# commonlit-Readability-competition
https://www.kaggle.com/competitions/commonlitreadabilityprize
First nlp project, learned and familiared with lots of new concepts and tools.

# My strategy 
Start from a roberta model pre-trained on CommonLit readability dataset on MaskedLanguageModeling task.
https://www.kaggle.com/datasets/maunish/clrp-roberta-base

and then fine-tuning to the train dataset.
Submitted with this single model and the public score is 0.468, the final rank is 866/3937.
